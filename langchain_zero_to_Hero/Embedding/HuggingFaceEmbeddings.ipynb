{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c5799378",
      "metadata": {
        "id": "c5799378"
      },
      "source": [
        "## Overview  \n",
        "- `Hugging Face` offers a wide range of **embedding models** for free, enabling various embedding tasks with ease.\n",
        "- In this tutorial, we‚Äôll use `langchain_huggingface` to build a **simple text embedding-based search system.**\n",
        "- The following models will be used for **Text Embedding**  \n",
        "\n",
        "    - 1Ô∏è‚É£ **multilingual-e5-large-instruct:** A multilingual instruction-based embedding model.  \n",
        "    - 2Ô∏è‚É£ **multilingual-e5-large:** A powerful multilingual embedding model.  \n",
        "    - 3Ô∏è‚É£ **bge-m3:** Optimized for large-scale text processing.  \n",
        "\n",
        "![](./assets/03-huggingfaceembeddings-workflow.png)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d94e164",
      "metadata": {
        "id": "3d94e164"
      },
      "source": [
        "### Table of Contents  \n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Data Preparation for Embedding-Based Search Tutorial](#data-preparation-for-embedding-based-search-tutorial)\n",
        "- [Which Text Embedding Model Should You Use?](#which-text-embedding-model-should-you-use)\n",
        "- [Similarity Calculation](#similarity-calculation)\n",
        "- [HuggingFaceEndpointEmbeddings Overview](#huggingfaceendpointembeddings-overview)\n",
        "- [HuggingFaceEmbeddings Overview](#huggingfaceembeddings-overview)\n",
        "- [FlagEmbedding Usage Guide](#flagembedding-usage-guide)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7a3526",
      "metadata": {
        "id": "1e7a3526"
      },
      "source": [
        "## Environment Setup  \n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.  \n",
        "\n",
        "**[Note]**  \n",
        "- `langchain-opentutorial` is a package that provides a set of **easy-to-use environment setup,** **useful functions,** and **utilities for tutorials.**  \n",
        "- You can check out the [`langchain-opentutorial` ](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.  \n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è **The following configurations will be set up**  \n",
        "\n",
        "- **Jupyter Notebook Output Settings**\n",
        "    - Display standard error ( `stderr` ) messages directly instead of capturing them.  \n",
        "- **Install Required Packages**\n",
        "    - Ensure all necessary dependencies are installed.  \n",
        "- **API Key Setup**\n",
        "    - Configure the API key for authentication.  \n",
        "- **PyTorch Device Selection Setup**\n",
        "    - Automatically select the optimal computing device (CPU, CUDA, or MPS).\n",
        "        - `{\"device\": \"mps\"}` : Perform embedding calculations using **MPS** instead of GPU. (For Mac users)\n",
        "        - `{\"device\": \"cuda\"}` : Perform embedding calculations using **GPU.** (For Linux and Windows users, requires CUDA installation)\n",
        "        - `{\"device\": \"cpu\"}` : Perform embedding calculations using **CPU.** (Available for all users)\n",
        "- **Embedding Model Local Storage Path**\n",
        "    - Define a local path for storing embedding models.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {
        "id": "21943adb"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {
        "id": "f25ec196"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain_huggingface\",\n",
        "        \"torch\",\n",
        "        \"numpy\",\n",
        "        \"scikit-learn\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7f9065ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f9065ea",
        "outputId": "0b2cfbed-6d18-4870-bc86-8fd3dee32fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"HuggingFace Embeddings\",  # Please set it the same as the title\n",
        "        \"HUGGINGFACEHUB_API_TOKEN\": \"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\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {
        "id": "690a9ae0"
      },
      "source": [
        "You can alternatively set OPENAI_API_KEY in `.env` file and load it.\n",
        "\n",
        "**[Note]**\n",
        "- This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4f99b5b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f99b5b6",
        "outputId": "a23a6a23-c7c4-446b-dcef-f64918f5c396"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IzdMoGDsGTB",
        "outputId": "bdbccd76-01ca-43f8-a5b5-e545221d85dd"
      },
      "id": "4IzdMoGDsGTB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "71b0e4a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71b0e4a1",
        "outputId": "7d728b57-36c7-4d58-ddaf-c24b07e14dc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using CUDA (NVIDIA GPU)\n",
            "üñ•Ô∏è Current device in use: cuda\n"
          ]
        }
      ],
      "source": [
        "# Automatically select the appropriate device\n",
        "import torch\n",
        "import platform\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    if platform.system() == \"Darwin\":  # macOS specific\n",
        "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "            print(\"‚úÖ Using MPS (Metal Performance Shaders) on macOS\")\n",
        "            return \"mps\"\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"‚úÖ Using CUDA (NVIDIA GPU)\")\n",
        "        return \"cuda\"\n",
        "    else:\n",
        "        print(\"‚úÖ Using CPU\")\n",
        "        return \"cpu\"\n",
        "\n",
        "\n",
        "# Set the device\n",
        "device = get_device()\n",
        "print(\"üñ•Ô∏è Current device in use:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "647c0c07",
      "metadata": {
        "id": "647c0c07"
      },
      "outputs": [],
      "source": [
        "# Embedding Model Local Storage Path\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set the download path to ./cache/\n",
        "os.environ[\"HF_HOME\"] = \"./cache/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7285cf35",
      "metadata": {
        "id": "7285cf35"
      },
      "source": [
        "## Data Preparation for Embedding-Based Search Tutorial\n",
        "\n",
        "To perform **embedding-based search,** we prepare both a **Query** and **Documents.**  \n",
        "\n",
        "1. Query  \n",
        "- Write a **key question** that will serve as the basis for the search.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2aeae907",
      "metadata": {
        "id": "2aeae907"
      },
      "outputs": [],
      "source": [
        "# Query\n",
        "q = \"Please tell me more about LangChain.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae9d0180",
      "metadata": {
        "id": "ae9d0180"
      },
      "source": [
        "2. Documents  \n",
        "- Prepare **multiple documents (texts)** that will serve as the target for the search.  \n",
        "- Each document will be **embedded** to enable semantic search capabilities.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0588d470",
      "metadata": {
        "id": "0588d470"
      },
      "outputs": [],
      "source": [
        "# Documents for Text Embedding\n",
        "docs = [\n",
        "    \"Hi, nice to meet you.\",\n",
        "    \"LangChain simplifies the process of building applications with large language models.\",\n",
        "    \"The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\",\n",
        "    \"LangChain simplifies the process of building applications with large-scale language models.\",\n",
        "    \"Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82008642",
      "metadata": {
        "id": "82008642"
      },
      "source": [
        "## Which Text Embedding Model Should You Use?\n",
        "- Leverage the **MTEB leaderboard** and **free embedding models** to confidently select and utilize the **best-performing text embedding models** for your projects! üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ **What is MTEB (Massive Text Embedding Benchmark)?**  \n",
        "- **MTEB** is a benchmark designed to **systematically and objectively evaluate** the performance of text embedding models.  \n",
        "    - **Purpose:** To **fairly compare** the performance of embedding models.  \n",
        "    - **Evaluation Tasks:** Includes tasks like **Classification,**  **Retrieval,**  **Clustering,**  and **Semantic Similarity.**  \n",
        "    - **Supported Models:** A wide range of **text embedding models available on Hugging Face.**  \n",
        "    - **Results:** Displayed as **scores,**  with top-performing models ranked on the **leaderboard.**  \n",
        "\n",
        "üîó [ **MTEB Leaderboard (Hugging Face)** ](https://huggingface.co/spaces/mteb/leaderboard)  \n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è **Models Used in This Tutorial**  \n",
        "\n",
        "| **Embedding Model** | **Description** |\n",
        "|----------|----------|\n",
        "| 1Ô∏è‚É£ **multilingual-e5-large-instruct** | Offers strong multilingual support with consistent results. |\n",
        "| 2Ô∏è‚É£ **multilingual-e5-large** | A powerful multilingual embedding model. |\n",
        "| 3Ô∏è‚É£ **bge-m3** | Optimized for large-scale text processing, excelling in retrieval and semantic similarity tasks. |\n",
        "\n",
        "1Ô∏è‚É£ **multilingual-e5-large-instruct**\n",
        "![](./assets/03-huggingfaceembeddings-leaderboard-01.png)\n",
        "\n",
        "2Ô∏è‚É£ **multilingual-e5-large**\n",
        "![](./assets/03-huggingfaceembeddings-leaderboard-02.png)\n",
        "\n",
        "3Ô∏è‚É£ **bge-m3**\n",
        "![](./assets/03-huggingfaceembeddings-leaderboard-03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "887b63bc",
      "metadata": {
        "id": "887b63bc"
      },
      "source": [
        "## Similarity Calculation\n",
        "\n",
        "**Similarity Calculation Using Vector Dot Product**  \n",
        "- Similarity is determined using the **dot product** of vectors.  \n",
        "\n",
        "- **Similarity Calculation Formula:**  \n",
        "\n",
        "$$ \\text{similarities} = \\mathbf{query} \\cdot \\mathbf{documents}^T $$  \n",
        "\n",
        "---\n",
        "\n",
        "### üìê **Mathematical Significance of the Vector Dot Product**  \n",
        "\n",
        "**Definition of Vector Dot Product**  \n",
        "\n",
        "The **dot product** of two vectors, $\\mathbf{a}$ and $\\mathbf{b}$, is mathematically defined as:  \n",
        "\n",
        "$$ \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i $$  \n",
        "\n",
        "---\n",
        "\n",
        "**Relationship with Cosine Similarity**  \n",
        "\n",
        "The **dot product** also relates to **cosine similarity** and follows this property:  \n",
        "\n",
        "$$ \\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos \\theta $$  \n",
        "\n",
        "Where:  \n",
        "- $\\|\\mathbf{a}\\|$ and $\\|\\mathbf{b}\\|$ represent the **magnitudes** (**norms,**  specifically Euclidean norms) of vectors $\\mathbf{a}$ and $\\mathbf{b}$.  \n",
        "- $\\theta$ is the **angle between the two vectors.**  \n",
        "- $\\cos \\theta$ represents the **cosine similarity** between the two vectors.  \n",
        "\n",
        "---\n",
        "\n",
        "**üîç Interpretation of Vector Dot Product in Similarity**  \n",
        "\n",
        "When the **dot product value is large** (a large positive value):  \n",
        "- The **magnitudes** ($\\|\\mathbf{a}\\|$ and $\\|\\mathbf{b}\\|$) of the two vectors are large.  \n",
        "- The **angle** ($\\theta$) between the two vectors is small ( **$\\cos \\theta$ approaches 1** ).  \n",
        "\n",
        "This indicates that the two vectors point in a **similar direction** and are **more semantically similar,**  especially when their magnitudes are also large.  \n",
        "\n",
        "---\n",
        "\n",
        "### üìè **Calculation of Vector Magnitude (Norm)**  \n",
        "\n",
        "**Definition of Euclidean Norm**  \n",
        "\n",
        "For a vector $\\mathbf{a} = [a_1, a_2, \\ldots, a_n]$, the **Euclidean norm** $\\|\\mathbf{a}\\|$ is calculated as:  \n",
        "\n",
        "$$ \\|\\mathbf{a}\\| = \\sqrt{a_1^2 + a_2^2 + \\cdots + a_n^2} $$  \n",
        "\n",
        "This **magnitude** represents the **length** or **size** of the vector in multi-dimensional space.  \n",
        "\n",
        "---\n",
        "\n",
        "Understanding these mathematical foundations helps ensure precise similarity calculations, enabling better performance in tasks like **semantic search,**  **retrieval systems,**  and **recommendation engines.**  üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf5d3874",
      "metadata": {
        "id": "bf5d3874"
      },
      "source": [
        "----\n",
        "### Similarity calculation between `embedded_query` and `embedded_document`\n",
        "- `embed_documents` : For embedding multiple texts (documents)\n",
        "- `embed_query` : For embedding a single text (query)\n",
        "\n",
        "We've implemented a method to search for the most relevant documents using **text embeddings.**\n",
        "- Let's use `search_similar_documents(q, docs, hf_embeddings)` to find the most relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f1e1a612",
      "metadata": {
        "id": "f1e1a612"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def search_similar_documents(q, docs, hf_embeddings):\n",
        "    # Embed the query and documents using the embedding model\n",
        "    embedded_query = hf_embeddings.embed_query(q)\n",
        "    embedded_documents = hf_embeddings.embed_documents(docs)\n",
        "\n",
        "    # Calculate similarity scores using dot product\n",
        "    similarity_scores = np.array(embedded_query) @ np.array(embedded_documents).T\n",
        "\n",
        "    # Sort documents by similarity scores in descending order\n",
        "    sorted_idx = similarity_scores.argsort()[::-1]\n",
        "\n",
        "    # Display the results\n",
        "    print(f\"[Query] {q}\\n\" + \"=\" * 40)\n",
        "    for i, idx in enumerate(sorted_idx):\n",
        "        print(f\"[{i}] {docs[idx]}\")\n",
        "        print()\n",
        "\n",
        "    # Return embeddings for potential further processing or analysis\n",
        "    return embedded_query, embedded_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a965e60",
      "metadata": {
        "id": "7a965e60"
      },
      "source": [
        "## HuggingFaceEndpointEmbeddings Overview\n",
        "\n",
        "**HuggingFaceEndpointEmbeddings** is a feature in the **LangChain** library that leverages **Hugging Face‚Äôs Inference API endpoint** to generate text embeddings seamlessly.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö **Key Concepts**\n",
        "\n",
        "1. **Hugging Face Inference API**  \n",
        "   - Access pre-trained embedding models via Hugging Face‚Äôs API.  \n",
        "   - No need to download models locally; embeddings are generated directly through the API.  \n",
        "\n",
        "2. **LangChain Integration**  \n",
        "   - Easily integrate embedding results into LangChain workflows using its standardized interface.  \n",
        "\n",
        "3. **Use Cases**  \n",
        "   - Text-query and document similarity calculation  \n",
        "   - Search and recommendation systems  \n",
        "   - Natural Language Understanding (NLU) applications  \n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Key Parameters**\n",
        "\n",
        "- `model` : The Hugging Face model ID (e.g., `BAAI/bge-m3` )  \n",
        "- `task` : The task to perform (usually `\"feature-extraction\"` )  \n",
        "- `api_key` : Your Hugging Face API token  \n",
        "- `model_kwargs` : Additional model configuration parameters  \n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Advantages**  \n",
        "- **No Local Model Download:** Instant access via API.  \n",
        "- **Scalability:** Supports a wide range of pre-trained Hugging Face models.  \n",
        "- **Seamless Integration:** Effortlessly integrates embeddings into LangChain workflows.  \n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Caveats**  \n",
        "- **API Support:** Not all models support API inference.  \n",
        "- **Speed & Cost:** Free APIs may have slower response times and usage limitations.  \n",
        "\n",
        "---\n",
        "\n",
        "With **HuggingFaceEndpointEmbeddings,**  you can easily integrate Hugging Face‚Äôs powerful embedding models into your **LangChain workflows** for efficient and scalable NLP solutions. üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd6c1ed1",
      "metadata": {
        "id": "bd6c1ed1"
      },
      "source": [
        "---\n",
        "Let‚Äôs use the `intfloat/multilingual-e5-large-instruct` model via the API to search for the most relevant documents using text embeddings.\n",
        "\n",
        "- [intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface==0.1.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cpwly0YNrGA",
        "outputId": "d6533e27-6288-496c-9f82-146fc35c5641"
      },
      "id": "0Cpwly0YNrGA",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-huggingface==0.1.2\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface==0.1.2) (0.29.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface==0.1.2) (0.3.47)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface==0.1.2) (3.4.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface==0.1.2) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface==0.1.2) (4.50.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.12.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.3.18)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.10.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (11.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (0.5.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.3.1)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, langchain-huggingface\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed langchain-huggingface-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ebeeab2c",
      "metadata": {
        "id": "ebeeab2c"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
        "\n",
        "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
        "\n",
        "hf_endpoint_embeddings = HuggingFaceEndpointEmbeddings(\n",
        "    model=model_name,\n",
        "    task=\"feature-extraction\",\n",
        "    huggingfacehub_api_token= \"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\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2f0d4ff",
      "metadata": {
        "id": "f2f0d4ff"
      },
      "source": [
        "Search for the most relevant documents based on a query using text embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f6910f88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6910f88",
        "outputId": "8f4165c6-6c45-40f6-b54c-17f07a43d556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.4 ms, sys: 918 ¬µs, total: 9.32 ms\n",
            "Wall time: 319 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Embed the query and documents using the embedding model\n",
        "embedded_query = hf_endpoint_embeddings.embed_query(q)\n",
        "embedded_documents = hf_endpoint_embeddings.embed_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "19ed03cf",
      "metadata": {
        "id": "19ed03cf"
      },
      "outputs": [],
      "source": [
        "# Calculate similarity scores using dot product\n",
        "similarity_scores = np.array(embedded_query) @ np.array(embedded_documents).T\n",
        "\n",
        "# Sort documents by similarity scores in descending order\n",
        "sorted_idx = similarity_scores.argsort()[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(embedded_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q82lju2tP6W6",
        "outputId": "d842b5c2-e65d-4f41-e613-dbfe2900ef81"
      },
      "id": "Q82lju2tP6W6",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00781461,  0.01651802, -0.00533472, ..., -0.03534667,\n",
              "       -0.00907558,  0.00625822])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuamUeq-P0Oy",
        "outputId": "13b4d3e1-406b-4c01-bdfe-92913d45221b"
      },
      "id": "wuamUeq-P0Oy",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.78163963, 0.88247226, 0.86764057, 0.87809576, 0.78404712])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2e288dbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e288dbd",
        "outputId": "3f4c6b1d-c7ea-4dfd-beb8-ebf0457411e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Query] Please tell me more about LangChain.\n",
            "========================================\n",
            "[0] LangChain simplifies the process of building applications with large language models.\n",
            "\n",
            "[1] LangChain simplifies the process of building applications with large-scale language models.\n",
            "\n",
            "[2] The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\n",
            "\n",
            "[3] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
            "\n",
            "[4] Hi, nice to meet you.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display the results\n",
        "print(f\"[Query] {q}\\n\" + \"=\" * 40)\n",
        "for i, idx in enumerate(sorted_idx):\n",
        "    print(f\"[{i}] {docs[idx]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665a8443",
      "metadata": {
        "id": "665a8443",
        "outputId": "cc72a7e8-193d-4267-b60a-9631a31add47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HuggingFace Endpoint Embedding]\n",
            "Model: \t\tintfloat/multilingual-e5-large-instruct\n",
            "Document Dimension: \t1024\n",
            "Query Dimension: \t1024\n"
          ]
        }
      ],
      "source": [
        "print(\"[HuggingFace Endpoint Embedding]\")\n",
        "print(f\"Model: \\t\\t{model_name}\")\n",
        "print(f\"Document Dimension: \\t{len(embedded_documents[0])}\")\n",
        "print(f\"Query Dimension: \\t{len(embedded_query)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e999ad0",
      "metadata": {
        "id": "3e999ad0"
      },
      "source": [
        "We can verify that the dimensions of `embedded_documents` and `embedded_query` are consistent.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1893bdf5",
      "metadata": {
        "id": "1893bdf5"
      },
      "source": [
        "You can also perform searches using the `search_similar_documents` method we implemented earlier.  \n",
        "From now on, let's use this method for our searches.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf3767e",
      "metadata": {
        "id": "1cf3767e",
        "outputId": "7bc9bfd8-9fa6-4991-dbb3-475d01e88b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Query] Please tell me more about LangChain.\n",
            "========================================\n",
            "[0] LangChain simplifies the process of building applications with large language models.\n",
            "\n",
            "[1] LangChain simplifies the process of building applications with large-scale language models.\n",
            "\n",
            "[2] The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\n",
            "\n",
            "[3] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
            "\n",
            "[4] Hi, nice to meet you.\n",
            "\n",
            "CPU times: user 7.25 ms, sys: 3.26 ms, total: 10.5 ms\n",
            "Wall time: 418 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "embedded_query, embedded_documents = search_similar_documents(q, docs, hf_endpoint_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e220112a",
      "metadata": {
        "id": "e220112a"
      },
      "source": [
        "## HuggingFaceEmbeddings Overview\n",
        "\n",
        "- **HuggingFaceEmbeddings** is a feature in the **LangChain** library that enables the conversion of text data into vectors using **Hugging Face embedding models.**\n",
        "- This class downloads and operates Hugging Face models **locally** for efficient processing.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö **Key Concepts**\n",
        "\n",
        "1. **Hugging Face Pre-trained Models**  \n",
        "   - Leverages pre-trained embedding models provided by Hugging Face.  \n",
        "   - Downloads models locally for direct embedding operations.  \n",
        "\n",
        "2. **LangChain Integration**  \n",
        "   - Seamlessly integrates with LangChain workflows using its standardized interface.  \n",
        "\n",
        "3. **Use Cases**  \n",
        "   - Text-query and document similarity calculation  \n",
        "   - Search and recommendation systems  \n",
        "   - Natural Language Understanding (NLU) applications  \n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Key Parameters**\n",
        "\n",
        "- `model_name` : The Hugging Face model ID (e.g., `sentence-transformers/all-MiniLM-L6-v2` )\n",
        "- `model_kwargs` : Additional model configuration parameters (e.g., GPU/CPU device settings)\n",
        "- `encode_kwargs` : Extra settings for embedding generation\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Advantages**  \n",
        "- **Local Embedding Operations:** Perform embeddings locally without requiring an internet connection.  \n",
        "- **High Performance:** Utilize GPU settings for faster embedding generation.  \n",
        "- **Model Variety:** Supports a wide range of Hugging Face models.  \n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Caveats**  \n",
        "- **Local Storage Requirement:** Pre-trained models must be downloaded locally.  \n",
        "- **Environment Configuration:** Performance may vary depending on GPU/CPU device settings.  \n",
        "\n",
        "---\n",
        "\n",
        "With **HuggingFaceEmbeddings,** you can efficiently leverage **Hugging Face's powerful embedding models** in a **local environment,** enabling flexible and scalable NLP solutions. üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f0bbecb",
      "metadata": {
        "id": "9f0bbecb"
      },
      "source": [
        "---\n",
        "Let's download the embedding model locally, perform embeddings, and search for the most relevant documents.\n",
        "\n",
        "`intfloat/multilingual-e5-large-instruct`\n",
        "\n",
        "- [intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c80d76",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "120f0abc878740eab866b3c6ac6174de",
            "576bde51a7e5429081fe8ca25be485a1",
            "b65bb288a10d4b6ca570b295df2e9489",
            "241ac0b490ad45ed82ab5166a03f0e42",
            "e65e9daa06854e619322a38b4d05a141",
            "f9274e892df24ad1999b5f5ccfd37111",
            "f5a6bdf6628d48eea1830e2d0a47f2f0",
            "07be2d5361824e2a84470111c25733fb",
            "393ae9475f554abeabd0387e7ede0141",
            "9f12edada76440809205236b96557ee7"
          ]
        },
        "id": "33c80d76",
        "outputId": "66bc954a-02e6-4e57-b1fe-bd796405ef41"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "120f0abc878740eab866b3c6ac6174de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "576bde51a7e5429081fe8ca25be485a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b65bb288a10d4b6ca570b295df2e9489",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/140k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "241ac0b490ad45ed82ab5166a03f0e42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e65e9daa06854e619322a38b4d05a141",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9274e892df24ad1999b5f5ccfd37111",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5a6bdf6628d48eea1830e2d0a47f2f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07be2d5361824e2a84470111c25733fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "393ae9475f554abeabd0387e7ede0141",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f12edada76440809205236b96557ee7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
        "\n",
        "hf_embeddings_e5_instruct = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={\"device\": device},  # mps, cuda, cpu\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ea2e5a",
      "metadata": {
        "id": "52ea2e5a",
        "outputId": "5b0bf92a-f162-40c1-8846-ab3ebbee5262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Query] Please tell me more about LangChain.\n",
            "========================================\n",
            "[0] LangChain simplifies the process of building applications with large language models.\n",
            "\n",
            "[1] LangChain simplifies the process of building applications with large-scale language models.\n",
            "\n",
            "[2] The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\n",
            "\n",
            "[3] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
            "\n",
            "[4] Hi, nice to meet you.\n",
            "\n",
            "CPU times: user 326 ms, sys: 120 ms, total: 446 ms\n",
            "Wall time: 547 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "embedded_query, embedded_documents = search_similar_documents(q, docs, hf_embeddings_e5_instruct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa283de3",
      "metadata": {
        "id": "fa283de3",
        "outputId": "d1d3834c-24a0-40a6-ce7c-b5cf9209a952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \t\tintfloat/multilingual-e5-large-instruct\n",
            "Document Dimension: \t1024\n",
            "Query Dimension: \t1024\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model: \\t\\t{model_name}\")\n",
        "print(f\"Document Dimension: \\t{len(embedded_documents[0])}\")\n",
        "print(f\"Query Dimension: \\t{len(embedded_query)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51605b3c",
      "metadata": {
        "id": "51605b3c"
      },
      "source": [
        "---\n",
        "`intfloat/multilingual-e5-large`\n",
        "\n",
        "- [intfloat/multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85d5b92",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9bf8485ade564180aa1fd64c933355e1",
            "44b9a9438711423cb9f685e4d52d53bc",
            "07eb4d424fa547f48dab5bdb64b7d25a",
            "2789d046962c4a72bab78ceaa0ab7f23",
            "b1b98e59b3db4b088163c9339033f3b3",
            "2865de5038644575874d6d869d0793e3",
            "5518d1906144480cbd8cbabec3c058de",
            "3c11fbd3dd3e43deadb605814d4b9437",
            "d39d895603ef40d78c42e4552867f7ad",
            "e3b5c216804c49758a4d079d1ef5273c"
          ]
        },
        "id": "c85d5b92",
        "outputId": "fed311a4-288d-4105-b3a9-72283fa10a6f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bf8485ade564180aa1fd64c933355e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44b9a9438711423cb9f685e4d52d53bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/160k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07eb4d424fa547f48dab5bdb64b7d25a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2789d046962c4a72bab78ceaa0ab7f23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1b98e59b3db4b088163c9339033f3b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2865de5038644575874d6d869d0793e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5518d1906144480cbd8cbabec3c058de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c11fbd3dd3e43deadb605814d4b9437",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d39d895603ef40d78c42e4552867f7ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3b5c216804c49758a4d079d1ef5273c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"intfloat/multilingual-e5-large\"\n",
        "\n",
        "hf_embeddings_e5_large = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={\"device\": device},  # mps, cuda, cpu\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42fb702b",
      "metadata": {
        "id": "42fb702b",
        "outputId": "b3a715fd-876c-41ab-81f2-e19f56d73854"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Query] Please tell me more about LangChain.\n",
            "========================================\n",
            "[0] LangChain simplifies the process of building applications with large-scale language models.\n",
            "\n",
            "[1] LangChain simplifies the process of building applications with large language models.\n",
            "\n",
            "[2] The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\n",
            "\n",
            "[3] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
            "\n",
            "[4] Hi, nice to meet you.\n",
            "\n",
            "CPU times: user 84.1 ms, sys: 511 ms, total: 595 ms\n",
            "Wall time: 827 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "embedded_query, embedded_documents = search_similar_documents(q, docs, hf_embeddings_e5_large)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb62912b",
      "metadata": {
        "id": "bb62912b",
        "outputId": "236a3398-0c87-4ca0-cfa0-12023f78c3fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \t\tintfloat/multilingual-e5-large\n",
            "Document Dimension: \t1024\n",
            "Query Dimension: \t1024\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model: \\t\\t{model_name}\")\n",
        "print(f\"Document Dimension: \\t{len(embedded_documents[0])}\")\n",
        "print(f\"Query Dimension: \\t{len(embedded_query)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69abf61b",
      "metadata": {
        "id": "69abf61b"
      },
      "source": [
        "---\n",
        "`BAAI/bge-m3`\n",
        "\n",
        "- [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac698306",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "220641d0c0a14704b59fb89a02990f32",
            "88cfaeafa8b5442bb3b207942f0605de",
            "a9ca58803e1c4d8181bc7104b688bfd7",
            "99e2b4fdd4434ea2bd3ab97b8bc9e5a0",
            "cf40cb0cb8cd4f8d8e61aa9e1bbc53a1",
            "629a48e3132846c0839b06d6bbb3c69e",
            "f427f87441bf40eb99ebf3516b812f3f",
            "9a70a867f5224a8e83ac08ae074c54e3",
            "10313c0b57304bb79ff97b04234d73f1",
            "9432edda58a84a5aa21e7e3dedc59bc6",
            "2007c23ff89542279a8b322f505190a9"
          ]
        },
        "id": "ac698306",
        "outputId": "261b30e8-24c5-41ab-c152-7825862eeaeb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "220641d0c0a14704b59fb89a02990f32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88cfaeafa8b5442bb3b207942f0605de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9ca58803e1c4d8181bc7104b688bfd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/15.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99e2b4fdd4434ea2bd3ab97b8bc9e5a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf40cb0cb8cd4f8d8e61aa9e1bbc53a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "629a48e3132846c0839b06d6bbb3c69e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f427f87441bf40eb99ebf3516b812f3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a70a867f5224a8e83ac08ae074c54e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10313c0b57304bb79ff97b04234d73f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9432edda58a84a5aa21e7e3dedc59bc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2007c23ff89542279a8b322f505190a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "model_kwargs = {\"device\": device}  # mps, cuda, cpu\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "hf_embeddings_bge_m3 = HuggingFaceEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a49ee5",
      "metadata": {
        "id": "53a49ee5",
        "outputId": "d3ac3ae6-df19-44cf-8c14-e6c5adadabcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Query] Please tell me more about LangChain.\n",
            "========================================\n",
            "[0] LangChain simplifies the process of building applications with large language models.\n",
            "\n",
            "[1] LangChain simplifies the process of building applications with large-scale language models.\n",
            "\n",
            "[2] The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\n",
            "\n",
            "[3] Hi, nice to meet you.\n",
            "\n",
            "[4] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
            "\n",
            "CPU times: user 81.1 ms, sys: 1.29 s, total: 1.37 s\n",
            "Wall time: 1.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "embedded_query, embedded_documents = search_similar_documents(q, docs, hf_embeddings_bge_m3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d1598f3",
      "metadata": {
        "id": "0d1598f3",
        "outputId": "2d4a6df3-8b74-44e5-d35e-c7f20547945c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \t\tBAAI/bge-m3\n",
            "Document Dimension: \t1024\n",
            "Query Dimension: \t1024\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model: \\t\\t{model_name}\")\n",
        "print(f\"Document Dimension: \\t{len(embedded_documents[0])}\")\n",
        "print(f\"Query Dimension: \\t{len(embedded_query)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42890bcc",
      "metadata": {
        "id": "42890bcc"
      },
      "source": [
        "## FlagEmbedding Usage Guide\n",
        "\n",
        "- **FlagEmbedding** is an advanced embedding framework developed by **BAAI (Beijing Academy of Artificial Intelligence).**\n",
        "- It supports **various embedding approaches** and is primarily used with the **BGE (BAAI General Embedding) model.**\n",
        "- FlagEmbedding excels in tasks such as **semantic search**, **natural language processing (NLP)**, and **recommendation systems.**\n",
        "\n",
        "---\n",
        "\n",
        "### üìö **Core Concepts of FlagEmbedding**\n",
        "\n",
        "1Ô∏è‚É£ `Dense Embedding`\n",
        "- Definition: Represents the overall meaning of a text as a single high-density vector.  \n",
        "- Advantages: Effectively captures semantic similarity.  \n",
        "- Use Cases: Semantic search, document similarity computation.  \n",
        "\n",
        "2Ô∏è‚É£ `Lexical Embedding`\n",
        "- Definition: Breaks text into word-level components, emphasizing word matching.  \n",
        "- Advantages: Ensures precise matching of specific words or phrases.  \n",
        "- Use Cases: Keyword-based search, exact word matching.  \n",
        "\n",
        "3Ô∏è‚É£ `Multi-Vector Embedding`\n",
        "- Definition: Splits a document into multiple vectors for representation.  \n",
        "- Advantages: Allows more granular representation of lengthy texts or diverse topics.  \n",
        "- Use Cases: Complex document structure analysis, detailed topic matching.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bb697bc",
      "metadata": {
        "id": "8bb697bc"
      },
      "source": [
        "---\n",
        "\n",
        "FlagEmbedding offers a **flexible and powerful toolkit** for leveraging embeddings across a wide range of **NLP tasks and semantic search applications.** üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60259624",
      "metadata": {
        "id": "60259624"
      },
      "source": [
        "The following code is used to control **tokenizer parallelism** in Hugging Face's `transformers` library:\n",
        "\n",
        "- `TOKENIZERS_PARALLELISM = \"true\"`  ‚Üí **Optimized for speed,** suitable for large-scale data processing.  \n",
        "- `TOKENIZERS_PARALLELISM = \"false\"`  ‚Üí **Ensures stability,** prevents conflicts and race conditions.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956c667a",
      "metadata": {
        "id": "956c667a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "726df20d",
      "metadata": {
        "id": "726df20d"
      },
      "outputs": [],
      "source": [
        "# install FlagEmbedding\n",
        "%pip install -qU FlagEmbedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8224874",
      "metadata": {
        "id": "a8224874"
      },
      "source": [
        "### ‚öôÔ∏è **Key Parameter**\n",
        "\n",
        "`BGEM3FlagModel`\n",
        "-  `model_name` : The Hugging Face **model ID** (e.g., `BAAI/bge-m3` ).\n",
        "-  `use_fp16` : When set to **True,** reduces **memory usage** and improves **encoding speed.**\n",
        "\n",
        "`bge_embeddings.encode`\n",
        "- `batch_size` : Defines the **number of documents** to process at once.  \n",
        "- `max_length` : Sets the **maximum token length** for encoding documents.  \n",
        "   - Increase for longer documents to ensure full content encoding.  \n",
        "   - Excessively large values may **degrade performance.**\n",
        "- `return_dense` : When set to **True**, returns **Dense Vectors** only.  \n",
        "- `return_sparse` : When set to **True**, returns **Sparse Vectors.**\n",
        "- `return_colbert_vecs` : When set to **True,** returns **ColBERT-style vectors.**\n",
        "\n",
        "\n",
        "\n",
        "### 1Ô∏è‚É£ **Dense Vector Embedding Example**\n",
        "- Definition: Represents the overall meaning of a text as a single high-density vector.  \n",
        "- Advantages: Effectively captures semantic similarity.  \n",
        "- Use Cases: Semantic search, document similarity computation.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b3624be",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "01bf90a4e376451fa42848cb9fdec85f",
            "74a97f05ce8a401eb895d9f38ced73e3",
            "069d0d9a20834a16b5c09e775d5a80ee",
            "5c2276dae01249148eeeea1486300d66",
            "4f88fbcea07a42f0b927c9b665b16d2c",
            "d9e85c188dd24f79bca12fe8927b2173",
            "b386913bf83643c6983cd356395bbcd5",
            "a0b6bd0ec09d4d8f9191a5b7ef599a4f",
            "bda3666fe0ef4f6a95906e67d60221cd",
            "1df70716e2f44988be1fb19faf7023da",
            "2d05eed702944f4f97ed62acee8ad3ad",
            "7caa9837bf7f4823a49dae66e9a254fc",
            "d5771da619ec403b8e3008900c37514f",
            "22b88cdb500742138861696050f1e101",
            "8ad27a74f07640668d44014e03435e7c",
            "5965f5ade52240648493251ba526c620",
            "700d9883252140d1bb593ec26be2ff37",
            "3fd00b14b2314048ba27713c3f7ae80e"
          ]
        },
        "id": "6b3624be",
        "outputId": "0d89ed56-ce9c-4188-8c1e-c4db024fcdcf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01bf90a4e376451fa42848cb9fdec85f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74a97f05ce8a401eb895d9f38ced73e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "imgs/mkqa.jpg:   0%|          | 0.00/608k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "069d0d9a20834a16b5c09e775d5a80ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "imgs/.DS_Store:   0%|          | 0.00/6.15k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c2276dae01249148eeeea1486300d66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "imgs/long.jpg:   0%|          | 0.00/485k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f88fbcea07a42f0b927c9b665b16d2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "imgs/bm25.jpg:   0%|          | 0.00/132k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9e85c188dd24f79bca12fe8927b2173",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "imgs/miracl.jpg:   0%|          | 0.00/576k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b386913bf83643c6983cd356395bbcd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "imgs/nqa.jpg:   0%|          | 0.00/158k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0b6bd0ec09d4d8f9191a5b7ef599a4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".gitattributes:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bda3666fe0ef4f6a95906e67d60221cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "colbert_linear.pt:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1df70716e2f44988be1fb19faf7023da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "imgs/others.webp:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d05eed702944f4f97ed62acee8ad3ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "long.jpg:   0%|          | 0.00/127k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7caa9837bf7f4823a49dae66e9a254fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "onnx/Constant_7_attr__value:   0%|          | 0.00/65.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5771da619ec403b8e3008900c37514f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "onnx/config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22b88cdb500742138861696050f1e101",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.onnx:   0%|          | 0.00/725k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ad27a74f07640668d44014e03435e7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.onnx_data:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5965f5ade52240648493251ba526c620",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "onnx/tokenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "700d9883252140d1bb593ec26be2ff37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fd00b14b2314048ba27713c3f7ae80e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sparse_linear.pt:   0%|          | 0.00/3.52k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "from FlagEmbedding import BGEM3FlagModel\n",
        "\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "\n",
        "bge_embeddings = BGEM3FlagModel(\n",
        "    model_name,\n",
        "    use_fp16=True,  # Enabling fp16 improves encoding speed with minimal precision trade-off.\n",
        ")\n",
        "\n",
        "# Encode documents with specified parameters\n",
        "embedded_documents_dense_vecs = bge_embeddings.encode(\n",
        "    sentences=docs,\n",
        "    batch_size=12,\n",
        "    max_length=8192,  # Reduce this value if your documents are shorter to speed up encoding.\n",
        ")[\"dense_vecs\"]\n",
        "\n",
        "# Query Encoding\n",
        "embedded_query_dense_vecs = bge_embeddings.encode(\n",
        "    sentences=[q],\n",
        "    batch_size=12,\n",
        "    max_length=8192,  # Reduce this value if your documents are shorter to speed up encoding.\n",
        ")[\"dense_vecs\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f3de31",
      "metadata": {
        "id": "23f3de31",
        "outputId": "5f8902f5-8c17-4e48-e430-8f2f64d60ac3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.0271  ,  0.003561, -0.0506  , ...,  0.00911 , -0.04565 ,\n",
              "         0.02028 ],\n",
              "       [-0.02242 , -0.01398 , -0.00946 , ...,  0.01851 ,  0.01907 ,\n",
              "        -0.01917 ],\n",
              "       [ 0.01386 , -0.02118 ,  0.01807 , ..., -0.01463 ,  0.04373 ,\n",
              "        -0.011856],\n",
              "       [-0.02365 , -0.008675, -0.000806, ...,  0.01537 ,  0.01438 ,\n",
              "        -0.02342 ],\n",
              "       [-0.01289 , -0.007313, -0.0121  , ..., -0.00561 ,  0.03787 ,\n",
              "         0.006016]], dtype=float16)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedded_documents_dense_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b63cf4a",
      "metadata": {
        "id": "2b63cf4a",
        "outputId": "7ba68b19-04bc-44f6-ccee-401dffff4d5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.02156 , -0.01993 , -0.01706 , ..., -0.01994 ,  0.0318  ,\n",
              "        -0.003395]], dtype=float16)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedded_query_dense_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410698d3",
      "metadata": {
        "id": "410698d3",
        "outputId": "b3c565d0-aa19-4878-a066-4508713a7161"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 1024)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# docs embedding dimension\n",
        "embedded_documents_dense_vecs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c160b8",
      "metadata": {
        "id": "33c160b8",
        "outputId": "b00366d6-b413-448b-cd3f-98f4cf8c6eb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 1024)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# query embedding dimension\n",
        "embedded_query_dense_vecs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6410ce6c",
      "metadata": {
        "id": "6410ce6c",
        "outputId": "405dfc0a-08da-4d21-9247-a0a4a23b9302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Please tell me more about LangChain.\n",
            "Most similar document: LangChain simplifies the process of building applications with large language models.\n"
          ]
        }
      ],
      "source": [
        "# Calculating Similarity Between Documents and Query\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarities = cosine_similarity(\n",
        "    embedded_query_dense_vecs, embedded_documents_dense_vecs\n",
        ")\n",
        "most_similar_idx = similarities.argmax()\n",
        "\n",
        "# Display the Most Similar Document\n",
        "print(f\"Question: {q}\")\n",
        "print(f\"Most similar document: {docs[most_similar_idx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dfb7c88",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "34fca22cdafa4ce4b01a1680405d35d0"
          ]
        },
        "id": "4dfb7c88",
        "outputId": "dcf529ab-fbd4-4b0f-fee2-ed042b670a2c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34fca22cdafa4ce4b01a1680405d35d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "from FlagEmbedding import BGEM3FlagModel\n",
        "\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "\n",
        "bge_embeddings = BGEM3FlagModel(\n",
        "    model_name,\n",
        "    use_fp16=True,  # Enabling fp16 improves encoding speed with minimal precision trade-off.\n",
        ")\n",
        "\n",
        "# Encode documents with specified parameters\n",
        "embedded_documents_dense_vecs_default = bge_embeddings.encode(\n",
        "    sentences=docs, return_dense=True\n",
        ")[\"dense_vecs\"]\n",
        "\n",
        "# Query Encoding\n",
        "embedded_query_dense_vecs_default = bge_embeddings.encode(\n",
        "    sentences=[q], return_dense=True\n",
        ")[\"dense_vecs\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d3b1e1",
      "metadata": {
        "id": "89d3b1e1",
        "outputId": "ef031cbf-0f34-431d-877f-04308a819e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Please tell me more about LangChain.\n",
            "Most similar document: LangChain simplifies the process of building applications with large language models.\n"
          ]
        }
      ],
      "source": [
        "# Calculating Similarity Between Documents and Query\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarities = cosine_similarity(\n",
        "    embedded_query_dense_vecs_default, embedded_documents_dense_vecs_default\n",
        ")\n",
        "most_similar_idx = similarities.argmax()\n",
        "\n",
        "# Display the Most Similar Document\n",
        "print(f\"Question: {q}\")\n",
        "print(f\"Most similar document: {docs[most_similar_idx]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd8142e",
      "metadata": {
        "id": "fbd8142e"
      },
      "source": [
        "### 2Ô∏è‚É£ **Sparse(Lexical) Vector Embedding Example**\n",
        "\n",
        "**Sparse Embedding (Lexical Weight)**\n",
        "- **Sparse embedding** is an embedding method that utilizes **high-dimensional vectors where most values are zero.**\n",
        "- The approach using **lexical weight** generates embeddings by considering the **importance of each word.**\n",
        "\n",
        "**How It Works**  \n",
        "1. Calculate the **lexical weight** for each word. Techniques like **TF-IDF** or **BM25** can be used.\n",
        "2. For each word in a document or query, assign a value to the corresponding dimension of the **sparse vector** based on its lexical weight.\n",
        "3. As a result, documents and queries are represented as **high-dimensional vectors where most values are zero.**\n",
        "\n",
        "**Advantages**  \n",
        "- Directly reflects the **importance of words.**\n",
        "- Enables **precise matching** of specific words or phrases.  \n",
        "- **Faster computation** compared to dense embeddings.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7fe20d7",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "757be6b2efce438ba041d6c21ed6d404"
          ]
        },
        "id": "c7fe20d7",
        "outputId": "bd7758fb-3ebd-43eb-bbb8-99acf3dc68aa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "757be6b2efce438ba041d6c21ed6d404",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "from FlagEmbedding import BGEM3FlagModel\n",
        "\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "\n",
        "bge_embeddings = BGEM3FlagModel(\n",
        "    model_name,\n",
        "    use_fp16=True,  # Enabling fp16 improves encoding speed with minimal precision trade-off.\n",
        ")\n",
        "\n",
        "# Encode documents with specified parameters\n",
        "embedded_documents_sparse_vecs = bge_embeddings.encode(\n",
        "    sentences=docs, return_sparse=True\n",
        ")\n",
        "\n",
        "# Query Encoding\n",
        "embedded_query_sparse_vecs = bge_embeddings.encode(sentences=[q], return_sparse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec7e781",
      "metadata": {
        "id": "6ec7e781"
      },
      "outputs": [],
      "source": [
        "lexical_scores_0 = bge_embeddings.compute_lexical_matching_score(\n",
        "    embedded_query_sparse_vecs[\"lexical_weights\"][0],\n",
        "    embedded_documents_sparse_vecs[\"lexical_weights\"][0],\n",
        ")\n",
        "\n",
        "lexical_scores_1 = bge_embeddings.compute_lexical_matching_score(\n",
        "    embedded_query_sparse_vecs[\"lexical_weights\"][0],\n",
        "    embedded_documents_sparse_vecs[\"lexical_weights\"][1],\n",
        ")\n",
        "\n",
        "lexical_scores_2 = bge_embeddings.compute_lexical_matching_score(\n",
        "    embedded_query_sparse_vecs[\"lexical_weights\"][0],\n",
        "    embedded_documents_sparse_vecs[\"lexical_weights\"][2],\n",
        ")\n",
        "\n",
        "lexical_scores_3 = bge_embeddings.compute_lexical_matching_score(\n",
        "    embedded_query_sparse_vecs[\"lexical_weights\"][0],\n",
        "    embedded_documents_sparse_vecs[\"lexical_weights\"][3],\n",
        ")\n",
        "\n",
        "lexical_scores_4 = bge_embeddings.compute_lexical_matching_score(\n",
        "    embedded_query_sparse_vecs[\"lexical_weights\"][0],\n",
        "    embedded_documents_sparse_vecs[\"lexical_weights\"][4],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e2e168f",
      "metadata": {
        "id": "1e2e168f",
        "outputId": "5625bcfd-c069-40b2-bc45-296ebdd04917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question: Please tell me more about LangChain.\n",
            "====================\n",
            "Hi, nice to meet you. : 0.0118865966796875\n",
            "LangChain simplifies the process of building applications with large language models. : 0.2313995361328125\n",
            "The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively. : 0.18797683715820312\n",
            "LangChain simplifies the process of building applications with large-scale language models. : 0.2268962860107422\n",
            "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses. : 0.002368927001953125\n"
          ]
        }
      ],
      "source": [
        "print(f\"question: {q}\")\n",
        "print(\"====================\")\n",
        "for i, doc in enumerate(docs):\n",
        "    print(doc, f\": {eval(f'lexical_scores_{i}')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f9079bb",
      "metadata": {
        "id": "3f9079bb"
      },
      "source": [
        "### 3Ô∏è‚É£ **Multi-Vector(ColBERT) Embedding Example**\n",
        "\n",
        "**ColBERT** (Contextualized Late Interaction over BERT) is an efficient approach for **document retrieval.**\n",
        "- This method uses a **multi-vector strategy** to represent both documents and queries with multiple vectors.  \n",
        "\n",
        "**How It Works**  \n",
        "1. Generate a **separate vector** for each **token in a document,** resulting in multiple vectors per document.  \n",
        "2. Similarly, generate a **separate vector** for each **token in a query.**\n",
        "3. During retrieval, calculate the **similarity** between each query token vector and all document token vectors.  \n",
        "4. Aggregate these similarity scores to produce a **final retrieval score.**  \n",
        "\n",
        "**Advantages**  \n",
        "- Enables **fine-grained token-level matching.**  \n",
        "- Captures **contextual embeddings** effectively.  \n",
        "- Performs efficiently even with **long documents.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca03e851",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f78fcf46adca4cbf868d684692e15b80"
          ]
        },
        "id": "ca03e851",
        "outputId": "3e1b8827-a69a-42c6-e650-0795e7e58cf2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f78fcf46adca4cbf868d684692e15b80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "from FlagEmbedding import BGEM3FlagModel\n",
        "\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "\n",
        "bge_embeddings = BGEM3FlagModel(\n",
        "    model_name,\n",
        "    use_fp16=True,  # Enabling fp16 improves encoding speed with minimal precision trade-off.\n",
        ")\n",
        "\n",
        "# Encode documents with specified parameters\n",
        "embedded_documents_colbert_vecs = bge_embeddings.encode(\n",
        "    sentences=docs, return_colbert_vecs=True\n",
        ")\n",
        "\n",
        "# Query Encoding\n",
        "embedded_query_colbert_vecs = bge_embeddings.encode(\n",
        "    sentences=[q], return_colbert_vecs=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e5b332f",
      "metadata": {
        "id": "7e5b332f"
      },
      "outputs": [],
      "source": [
        "colbert_scores_0 = bge_embeddings.colbert_score(\n",
        "    embedded_query_colbert_vecs[\"colbert_vecs\"][0],\n",
        "    embedded_documents_colbert_vecs[\"colbert_vecs\"][0],\n",
        ")\n",
        "\n",
        "colbert_scores_1 = bge_embeddings.colbert_score(\n",
        "    embedded_query_colbert_vecs[\"colbert_vecs\"][0],\n",
        "    embedded_documents_colbert_vecs[\"colbert_vecs\"][1],\n",
        ")\n",
        "\n",
        "colbert_scores_2 = bge_embeddings.colbert_score(\n",
        "    embedded_query_colbert_vecs[\"colbert_vecs\"][0],\n",
        "    embedded_documents_colbert_vecs[\"colbert_vecs\"][2],\n",
        ")\n",
        "\n",
        "colbert_scores_3 = bge_embeddings.colbert_score(\n",
        "    embedded_query_colbert_vecs[\"colbert_vecs\"][0],\n",
        "    embedded_documents_colbert_vecs[\"colbert_vecs\"][3],\n",
        ")\n",
        "\n",
        "colbert_scores_4 = bge_embeddings.colbert_score(\n",
        "    embedded_query_colbert_vecs[\"colbert_vecs\"][0],\n",
        "    embedded_documents_colbert_vecs[\"colbert_vecs\"][4],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cee99b2",
      "metadata": {
        "id": "1cee99b2",
        "outputId": "e878695b-58b7-4b3e-8767-e9eed712310e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question: Please tell me more about LangChain.\n",
            "====================\n",
            "Hi, nice to meet you. : 0.509117841720581\n",
            "LangChain simplifies the process of building applications with large language models. : 0.7039894461631775\n",
            "The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively. : 0.6632840037345886\n",
            "LangChain simplifies the process of building applications with large-scale language models. : 0.7057777643203735\n",
            "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses. : 0.38082367181777954\n"
          ]
        }
      ],
      "source": [
        "print(f\"question: {q}\")\n",
        "print(\"====================\")\n",
        "for i, doc in enumerate(docs):\n",
        "    print(doc, f\": {eval(f'colbert_scores_{i}')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d8d5f52",
      "metadata": {
        "id": "3d8d5f52"
      },
      "source": [
        "### üí° **Advantages of FlagEmbedding**  \n",
        "\n",
        "- **Diverse Embedding Options:** Supports the **Dense,** **Lexical,** and **Multi-Vector** approaches.  \n",
        "- **High-Performance Models:** Utilizes powerful pre-trained models like **BGE.**  \n",
        "- **Flexibility:** Choose the optimal embedding method based on your **use case.**  \n",
        "- **Scalability:** Capable of performing embeddings on **large-scale datasets.**  \n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Considerations**  \n",
        "\n",
        "- **Model Size:** Some models may require **significant storage capacity.**  \n",
        "- **Resource Requirements:** **GPU usage is recommended** for large-scale vector computations.  \n",
        "- **Configuration Needs:** Optimal performance may require **parameter tuning.**   \n",
        "\n",
        "---\n",
        "\n",
        "### üìä **FlagEmbedding Vector Comparison**  \n",
        "\n",
        "| **Embedding Type** | **Strengths**         | **Use Cases**              |\n",
        "|---------------------|-----------------------|----------------------------|\n",
        "| **Dense Vector**   | Emphasizes semantic similarity | Semantic search, document matching |\n",
        "| **Lexical Vector** | Precise word matching        | Keyword search, exact matches      |\n",
        "| **Multi-Vector**   | Captures complex meanings    | Long document analysis, topic classification |\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}